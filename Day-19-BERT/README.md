# Day 19 - BERT & Encoder Models
 
 **Topics Covered:** Encoder-only Architecture, Bidirectionality, Masked Language Modeling (MLM), Next Sentence Prediction (NSP), Fine-tuning, Tokenization (CLS/SEP)
 
 ---
 
 ## Question 1: BERT Architecture
 
 **Topic:** Architecture
 **Difficulty:** Basic
 
 ### Question
 What does BERT stand for? Which part of the Transformer does it use?
 
 ### Answer
 
 **BERT:** **B**idirectional **E**ncoder **R**epresentations from **T**ransformers.
 
 **Architecture:**
 - It uses **only the Encoder stack** of the Transformer.
 - It is designed to understand text (NLU), not to generate text.
 - **Size:**
    - BERT-Base: 12 Layers (110M params).
    - BERT-Large: 24 Layers (340M params).
 
 ---
 
 ## Question 2: "Bidirectional" in BERT
 
 **Topic:** Concept
 **Difficulty:** Intermediate
 
 ### Question
 Traditional language models (like GPT or RNNs) are unidirectional (left-to-right). How is BERT truly bidirectional?
 
 ### Answer
 
 **Unidirectional (GPT/RNN):** To understand word $x_t$, the model can only look at $x_1...x_{t-1}$.
 
 **Bidirectional (BERT):**
 - Because BERT uses **Masked Language Modeling (MLM)** (cloze task) instead of Next Word Prediction, "Mask" allows the model to see the entire sentence at once.
 - To understand "bank" in "I went to the **bank** to fish", it sees both "went to" (past) and "to fish" (future) simultaneously.
 - This context from both sides is crucial for disambiguation.
 
 ---
 
 ## Question 3: Masked Language Modeling (MLM)
 
 **Topic:** Pre-training
 **Difficulty:** Intermediate
 
 ### Question
 Explain the MLM pre-training objective. Why use [MASK] instead of just deleting the word?
 
 ### Answer
 
 **Objective:**
 1. Randomly hide 15% of the tokens in the input.
 2. Replace them with the `[MASK]` token (80% of time), random word (10%), or keep original (10%).
 3. Ask the model to predict the original word based on context.
 
 **Why not delete?**
 - The model needs to know *that* a word exists at that position to maintain positional relationships.
 - The `[MASK]` token acts as a placeholder for the model to focus its attention on.
 
 ---
 
 ## Question 4: Next Sentence Prediction (NSP)
 
 **Topic:** Pre-training
 **Difficulty:** Intermediate
 
 ### Question
 What is NSP? Why was it included in BERT's training?
 
 ### Answer
 
 **Objective:**
 - Input: `[CLS] Sentence A [SEP] Sentence B`.
 - Task: Binary classification. Is Sentence B the actual next sentence after A? (IsNext / NotNext).
 
 **Why?**
 - To teach the model relationships **between sentences**.
 - Critical for downstream tasks like Question Answering (Question -> Answer) or Natural Language Inference (Premise -> Hypothesis).
 - *Note: Later models like RoBERTa showed NSP wasn't actually that necessary.*
 
 ---
 
 ## Question 5: Input Representation
 
 **Topic:** Architecture
 **Difficulty:** Intermediate
 
 ### Question
 BERT inputs are the sum of three embeddings. What are they?
 
 ### Answer
 
 1. **Token Embeddings:** The vector for the specific word/subword (WordPiece).
 2. **Segment Embeddings:** Indicates if the token belongs to Sentence A or Sentence B.
 3. **Position Embeddings:** Different from original sine/cosine, BERT learns static position vectors for indices 0 to 511.
 
 **Final Input:** `Token + Segment + Position`.
 
 ---
 
 ## Question 6: Fine-Tuning
 
 **Topic:** Application
 **Difficulty:** Basic
 
 ### Question
 What is "Fine-Tuning"? How do you adapt BERT for Sentiment Analysis?
 
 ### Answer
 
 **Fine-Tuning:**
 - Taking a pre-trained model (trained on Wikipedia/Books) and training it further on your specific dataset (e.g., Movie Reviews).
 
 **Process for Sentiment Analysis:**
 1. Add a simple **Classification Layer** (Feed Forward + Softmax) on top of the `[CLS]` token output.
 2. `[CLS]` is a special token prepended to every input that aggregates the entire sequence representation.
 3. Feed your labeled data. Update weights of the whole model (BERT + Classifier) using a small learning rate.
 
 ---
 
 ## Question 7: Contextualized Embeddings
 
 **Topic:** Comparisons
 **Difficulty:** Intermediate
 
 ### Question
 How does the vector for the word "Apple" from BERT differ from "Apple" in Word2Vec?
 
 ### Answer
 
 - **Word2Vec (Static):** There is one single vector for "Apple" in the lookup table. It mixes "Fruit" and "Tech Company" meanings permanently.
 - **BERT (Contextual):**
    - The vector is generated by passing the sentence through 12 layers of Attention.
    - "Eat an **Apple**": The vector will be close to "Pear".
    - "**Apple** iPhone": The vector will be close to "Microsoft".
    - BERT generates a unique vector for every *instance* of a word.
 
 ---
 
 ## Question 8: BERT Variants
 
 **Topic:** General Knowledge
 **Difficulty:** Basic
 
 ### Question
 Briefly explain DistilBERT and RoBERTa.
 
 ### Answer
 
 1. **RoBERTa (Robustly optimized BERT):**
    - Facebook's improvement on BERT.
    - Removed NSP task.
    - Trained on way more data for longer.
    - Dynamically changes masking pattern.
    - Result: Better performance.
 
 2. **DistilBERT:**
    - A smaller, faster, cheaper version of BERT.
    - Trained using "Distillation" (Teacher-Student).
    - Retains 97% of performance but is 40% smaller and 60% faster.
 
 ---
 
 ## Question 9: Why 512 Tokens?
 
 **Topic:** Architecture Limiation
 **Difficulty:** Advanced
 
 ### Question
 BERT has a max sequence length of 512 tokens. Why this specific limit?
 
 ### Answer
 
 - **Quadratic Complexity ($O(n^2)$):**
 - Self-Attention requires calculating an $n \times n$ matrix ($Q \cdot K^T$).
 - Doubling input length (512 -> 1024) quadruples the memory and compute.
 - 512 was chosen as a trade-off between covering most paragraphs and keeping training feasible on 2018 hardware.
 
 ---
 
 ## Question 10: Using BERT for QA
 
 **Topic:** Application
 **Difficulty:** Advanced
 
 ### Question
 How is BERT architected for Question Answering (SQuAD)?
 
 ### Answer
 
 - **Input:** `[CLS] Question [SEP] Context Paragraph`.
 - **Output:** Two vectors: Start Logits and End Logits.
 - The model predicts two indices in the Context Paragraph:
    1. Where the answer starts.
    2. Where the answer ends.
 - It essentially performs "Span Extraction" from the text.
 
 ---
 
 ## Key Takeaways
 
 - **BERT** (Encoder) changed NLP by bringing **Transfer Learning** (Pre-train -> Fine-tune).
 - **Bidirectionality** allows deep understanding of context.
 - **[CLS]** token is used for classification; **[SEP]** separates sentences.
 - **Masked Language Modeling** enables the model to see the future tokens during training.
 
 **Next:** [Day 20 - GPT & Decoders](../Day-20/README.md)
